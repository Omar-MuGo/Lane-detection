{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1838c23b-3963-49f6-a9c4-b4f471a71e9e",
   "metadata": {},
   "source": [
    "# Lane detection with CARLA\n",
    "\n",
    "Author: Omar Rodrigo Muñoz Gómez\n",
    "Date: 2022/11/20\n",
    "\n",
    "The code creates a vehicle, attaches a camera to it and feeds the video to a UNET for semantic segmentation of the vehicle lanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a864e1f6-8a68-416d-b7af-97e7e514efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889a337-ae5d-4133-a207-240b433ffcbe",
   "metadata": {},
   "source": [
    "Define the virtual world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74e0c7a-e65b-4121-b80a-6200fd4c33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client to connect to CARLA server\n",
    "client = carla.Client('localhost', 2000)\n",
    "\n",
    "# Create a world object to access things within the simulation\n",
    "town_string = \"Town04\"\n",
    "client.load_world(town_string)\n",
    "world = client.get_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0faac1ef-0b7f-4d89-ba79-1c9138f92a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blue prints give us access to the blueprints needed to create objects\n",
    "# within CARLA (pedestrians, trees, etc).\n",
    "bp_lib = world.get_blueprint_library()\n",
    "\n",
    "# Retrieve the predefined spawn points from the map\n",
    "spawn_points = world.get_map().get_spawn_points()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd797fe-bd90-4df4-8b05-bde6888c2ced",
   "metadata": {},
   "source": [
    "Create a vehicle in the virtual world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22d8a82-23c9-426c-9a52-b39313e7dafe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose a vehicle blueprint and spawn the vehicle at a random location\n",
    "vehicle_bp = bp_lib.find('vehicle.tesla.model3')\n",
    "vehicle    = world.try_spawn_actor(vehicle_bp, random.choice(spawn_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2a9836-04ce-44cd-a7af-831732f661ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position the spectator behind the newly created vehicle\n",
    "spectator = world.get_spectator()\n",
    "transform = carla.Transform(vehicle.get_transform().transform(carla.Location(x=-5, z=2.5)), vehicle.get_transform().rotation)\n",
    "spectator.set_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443f017-68eb-4d76-be91-57186824e299",
   "metadata": {},
   "source": [
    "Set the vehicle into autopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "613e3d57-8d3a-4cf1-b7b8-51cb2641cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the autopilot for the vehicles in the world\n",
    "for v in world.get_actors().filter('vehicle.*.*'):\n",
    "    v.set_autopilot(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98795fb-afb4-49ae-80a8-6cc14f5dc277",
   "metadata": {},
   "source": [
    "Attach a camera into the vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affc0c33-02ab-4ad0-8a26-77d988854986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a camera\n",
    "camera_bp         = bp_lib.find('sensor.camera.rgb')\n",
    "camera_bp.set_attribute('image_size_x', '256')\n",
    "camera_bp.set_attribute('image_size_y', '128')\n",
    "\n",
    "\n",
    "camera_init_trans = carla.Transform(carla.Location(z=1.6, x=0.4))\n",
    "camera            = world.spawn_actor(camera_bp, camera_init_trans, attach_to=vehicle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b9685d-5929-4d3e-8099-3fe2e66e4f3c",
   "metadata": {},
   "source": [
    "Store the last frame into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2156d6-90bd-4d60-9022-668bd351391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_callback(image, data_dict):\n",
    "    data_dict['rgb_img'] = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20377468-86c4-4800-9297-9600b72026d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_w = camera_bp.get_attribute(\"image_size_x\").as_int()\n",
    "image_h = camera_bp.get_attribute(\"image_size_y\").as_int()\n",
    "\n",
    "camera_data = {'rgb_img': np.zeros((image_h, image_w, 4))}\n",
    "camera.listen(lambda image: camera_callback(image, camera_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5822e-f1f3-4f5b-9661-3cafcd5960d9",
   "metadata": {},
   "source": [
    "Optionally preview the image from the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da8ceb07-2604-410a-921d-e4646f6927e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream the camera feed into an opencv window\n",
    "cv2.namedWindow('RGB Camera', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('RGB Camera', camera_data['rgb_img'])\n",
    "cv2.waitKey(1)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow('RGB Camera', camera_data['rgb_img'])\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c1302e-f77f-4d50-8987-f02303ae35ef",
   "metadata": {},
   "source": [
    "Load the pretrained UNET model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26514c7d-0987-456d-b0fc-ace7ccf1cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from skimage import color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6e8935-a6bc-464e-89f1-a0e2538f758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dcc42a-af5e-4a50-bf07-9020ed25ba66",
   "metadata": {},
   "source": [
    "Make predictions over the live video feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d1419c-90f3-460f-87b0-025fdbf2cf6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "# Resize images to a smaller size\n",
    "SIZE_X = 128 \n",
    "SIZE_Y = 128\n",
    "\n",
    "# Stream the camera feed into an opencv window\n",
    "cv2.namedWindow('Gray Camera', cv2.WINDOW_AUTOSIZE)\n",
    "gray = cv2.cvtColor(camera_data['rgb_img'], cv2.COLOR_RGB2GRAY)\n",
    "cv2.imshow('Gray Camera', gray)\n",
    "cv2.waitKey(1)\n",
    "\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    i = i + 1\n",
    "    \n",
    "    \n",
    "    gray = cv2.cvtColor(camera_data['rgb_img'], cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    img = cv2.resize(gray, (SIZE_X, SIZE_Y))\n",
    "    img = tf.cast(img, tf.float32) / 255.0 # Normalize\n",
    "    img = np.array(img)\n",
    "    \n",
    "    y_pred       = my_model.predict(np.expand_dims(img,axis=0))\n",
    "    y_pred_argmax= np.argmax(y_pred, axis=3)\n",
    "    res = y_pred_argmax[0]\n",
    "    \n",
    "    result_image = color.label2rgb(res, img)    \n",
    "    \n",
    "    cv2.imshow('Gray Camera', result_image) # FIXME: Use correct colormap to display\n",
    "    result_image = cv2.convertScaleAbs(result_image, alpha=(255.0))\n",
    "    cv2.imwrite('prediction_'+str(i)+'.png', result_image)\n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01a453-90e1-443a-b4be-d919c3f29885",
   "metadata": {},
   "source": [
    "Optionally, visualize the results of a single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea8a265-433f-49b9-beef-57fdb97e107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    }
   ],
   "source": [
    "# Resize images to a smaller size\n",
    "SIZE_X = 128 \n",
    "SIZE_Y = 128\n",
    "\n",
    "gray = cv2.cvtColor(camera_data['rgb_img'], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "img = cv2.resize(gray, (SIZE_X, SIZE_Y))\n",
    "img = tf.cast(img, tf.float32) / 255.0 # Normalize\n",
    "img = np.array(img)\n",
    "\n",
    "y_pred       = my_model.predict(np.expand_dims(img,axis=0))\n",
    "y_pred_argmax= np.argmax(y_pred, axis=3)\n",
    "res = y_pred_argmax[0]\n",
    "\n",
    "result_image = color.label2rgb(res, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3868c97-2557-413d-9c19-1f71d4414786",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('sample image',result_image)\n",
    " \n",
    "cv2.waitKey(0) # waits until a key is pressed\n",
    "cv2.destroyAllWindows() # destroys the window showing image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
